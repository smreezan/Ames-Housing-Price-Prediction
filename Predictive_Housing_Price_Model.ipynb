{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ca4a1a",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12239db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import shap\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca294acf",
   "metadata": {},
   "source": [
    "# Import data from csv file and peak at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b80ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\") \n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83eaf8f",
   "metadata": {},
   "source": [
    "##  Checking the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321cec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b981ac",
   "metadata": {},
   "source": [
    "# Preprocessing and Model Technical Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6455fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 0: FORCE RESET & DATA LOAD\n",
    "# ==============================================================================\n",
    "# \"train.csv\" assumes the data file is in the SAME folder as this notebook\n",
    "df = pd.read_csv(\"train.csv\") \n",
    "\n",
    "print(f\"Data Reset Successful. Ready to process {len(df)} houses.\")\n",
    "print(f\"Initial Mean Price: ${df['SalePrice'].mean():,.2f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 1: FEATURE ENGINEERING\n",
    "# ==============================================================================\n",
    "df['HighValueSF'] = df['GrLivArea'] + (df['TotalBsmtSF'] * 0.5)\n",
    "df['LuxuryIndex'] = (df['OverallQual'] * df['YearBuilt']) / 1000\n",
    "\n",
    "# Neighborhood Metrics \n",
    "neigh_map = df.groupby('Neighborhood')['SalePrice'].median().to_dict()\n",
    "df['Neigh_Richness'] = df['Neighborhood'].map(neigh_map)\n",
    "\n",
    "neigh_pps = df.groupby('Neighborhood').apply(\n",
    "    lambda x: (x['SalePrice'] / x['GrLivArea']).median(), \n",
    "    include_groups=False\n",
    ").to_dict()\n",
    "df['Neigh_PPSF'] = df['Neighborhood'].map(neigh_pps)\n",
    "\n",
    "df['Quality_Power'] = (df['OverallQual'] ** 2) * df['GrLivArea']\n",
    "df['TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n",
    "df['TotalBath'] = df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath'])\n",
    "df['HouseAge'] = (df['YrSold'] - df['YearBuilt']).clip(lower=0)\n",
    "df['Total_Score'] = df['OverallQual'] * df['GrLivArea']\n",
    "\n",
    "# Log-transform skewed features\n",
    "skew_cols = ['LotArea', 'GrLivArea', 'TotalSF', 'HighValueSF', 'Neigh_Richness', \n",
    "             'Total_Score', 'Neigh_PPSF', 'Quality_Power']\n",
    "for col in skew_cols:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 2: OUTLIER REMOVAL\n",
    "# ==============================================================================\n",
    "initial_len = len(df)\n",
    "df = df.drop(df[(df['GrLivArea'] > 4000) & (df['SalePrice'] < 300000)].index, errors='ignore')\n",
    "\n",
    "df['PricePerSF'] = df['SalePrice'] / df['GrLivArea']\n",
    "low_q, high_q = df['PricePerSF'].quantile(0.03), df['PricePerSF'].quantile(0.97)\n",
    "df = df[(df['PricePerSF'] > low_q) & (df['PricePerSF'] < high_q)]\n",
    "df = df.drop(columns=['PricePerSF'])\n",
    "\n",
    "for col in ['GrLivArea', 'TotalSF', 'Total_Score', 'LotArea']:\n",
    "    if col in df.columns:\n",
    "        m, s = df[col].mean(), df[col].std()\n",
    "        df = df[(df[col] > (m - 2*s)) & (df[col] < (m + 2*s))]\n",
    "print(f\"Removed {initial_len - len(df)} outliers.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 3: LOG TRANSFORM TARGET\n",
    "# ==============================================================================\n",
    "y_final = np.log1p(df['SalePrice']) \n",
    "X_final = df.drop('SalePrice', axis=1)\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 4: PIPELINE SETUP\n",
    "# ==============================================================================\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)\n",
    "\n",
    "qual_levels = ['None', 'Po', 'Fa', 'TA', 'Gd', 'Ex']\n",
    "ordinal_categories = [qual_levels]*3 + [['None','No','Mn','Av','Gd']] + [['None','Unf','LwQ','Rec','BLQ','ALQ','GLQ']]*2 + [qual_levels] + [['None','Unf','RFn','Fin']] + [qual_levels]*2\n",
    "\n",
    "ordinal_features = ['PoolQC', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "numeric_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "nominal_features = [c for c in X_train.select_dtypes(include=['object']).columns if c not in ordinal_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', Pipeline([('imp', SimpleImputer(strategy='median')), ('scal', StandardScaler())]), numeric_features),\n",
    "    ('ord', Pipeline([('imp', SimpleImputer(strategy='constant', fill_value='None')), ('enc', OrdinalEncoder(categories=ordinal_categories))]), ordinal_features),\n",
    "    ('nom', Pipeline([('imp', SimpleImputer(strategy='constant', fill_value='None')), ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))]), nominal_features)\n",
    "])\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 5: DEFINE MODELS \n",
    "# ==============================================================================\n",
    "ridge_model = Ridge(alpha=15)\n",
    "rf_model = RandomForestRegressor(n_estimators=500, max_depth=15, random_state=42)\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.01, max_depth=3, loss='huber', random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Linear_Ridge': ridge_model,\n",
    "    'Random_Forest': rf_model,\n",
    "    'Gradient_Boosting_Huber': gbr_model,\n",
    "    'Safety_Ensemble': VotingRegressor(estimators=[('ridge', ridge_model), ('gbr', gbr_model)], weights=[0.5, 0.5])\n",
    "}\n",
    "\n",
    "# ==============================================================================\n",
    "# STEP 6: FINAL EVALUATION\n",
    "# ==============================================================================\n",
    "TARGET_RMSE = 17699.59\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"             EVALUATION LEADERBOARD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred_log = pipeline.predict(X_test)\n",
    "    \n",
    "    # REVERSE LOGS TO GET REAL DOLLARS\n",
    "    actuals = np.expm1(y_test)\n",
    "    preds = np.expm1(y_pred_log)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(actuals, preds))\n",
    "    mape = mean_absolute_percentage_error(actuals, preds) * 100\n",
    "    \n",
    "    status = \" TARGET ACHIEVED\" if (rmse < TARGET_RMSE) else \" BASELINE\"\n",
    "    print(f\"{name:.<30} RMSE: ${rmse:,.2f} | MAPE: {mape:.2f}% | {status}\")\n",
    "\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812dd79a",
   "metadata": {},
   "source": [
    "# Evaluate feature importance using SHAP values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Access the trained Gradient Boosting model from your ensemble\n",
    "# In a VotingRegressor, estimators_[1] is the GradientBoostingRegressor\n",
    "best_model = models['Safety_Ensemble'].estimators_[1]\n",
    "\n",
    "# 2. Preprocess the X_test data so it is purely numerical\n",
    "# Using the 'preprocessor' was defined in a pipeline earlier\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame to keep feature names in the plot\n",
    "# 'current_feature_names' was generated in the Task 2 fixed script\n",
    "X_test_df = pd.DataFrame(X_test_transformed, columns=current_feature_names)\n",
    "\n",
    "# 3. Calculate SHAP values\n",
    "explainer = shap.TreeExplainer(best_model)\n",
    "shap_values = explainer.shap_values(X_test_df)\n",
    "\n",
    "# 4. Generate the SHAP Summary Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_df, show=False)\n",
    "\n",
    "# 5. Professional Styling and Export\n",
    "plt.title('Feature Impact Analysis: SHAP Global Summary', fontsize=15, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the high-resolution version for a report\n",
    "plt.savefig('images/shap_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization successfully saved as 'images/shap_feature_importance.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fd16e9",
   "metadata": {},
   "source": [
    "# Feature importance plots and prediction vs actual plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96508be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for the report\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# Plot 1: Actual vs Predicted (The 'Accuracy' proof)\n",
    "sns.scatterplot(x=actuals, y=preds, ax=axes[0], alpha=0.5, color='teal')\n",
    "axes[0].plot([actuals.min(), actuals.max()], [actuals.min(), actuals.max()], 'r--', lw=2)\n",
    "axes[0].set_title('Actual vs. Predicted Prices', fontsize=14)\n",
    "axes[0].set_xlabel('Actual Price ($)')\n",
    "axes[0].set_ylabel('Predicted Price ($)')\n",
    "\n",
    "# Plot 2: Top 10 Feature Importance (The 'Why' proof)\n",
    "# Using Gradient Boosting component for importances\n",
    "importances = models['Safety_Ensemble'].estimators_[1].feature_importances_\n",
    "feat_importances = pd.Series(importances, index=all_feature_names)\n",
    "feat_importances.nlargest(10).plot(kind='barh', ax=axes[1], color='navy')\n",
    "axes[1].set_title('Top 10 Price Drivers', fontsize=14)\n",
    "axes[1].invert_yaxis() # Highest importance on top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daec176",
   "metadata": {},
   "source": [
    "# Exporting cleaned data, predictions, and feature scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3cf7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GENERATE FEATURE NAMES \n",
    "ohe_feature_names = preprocessor.transformers_[2][1].named_steps['ohe'].get_feature_names_out(nominal_features)\n",
    "current_feature_names = numeric_features + ordinal_features + list(ohe_feature_names)\n",
    "\n",
    "# 2. Create a \"Cleaned_Dataset_With_Predictions\" file\n",
    "cleaned_output = X_test.copy()\n",
    "\n",
    "# Reverse the log transformation for the features were transformed in Step 1\n",
    "for col in skew_cols:\n",
    "    if col in cleaned_output.columns:\n",
    "        cleaned_output[col] = np.expm1(cleaned_output[col])\n",
    "\n",
    "# Add the Target and the Predictions (Reversing logs for dollars)\n",
    "cleaned_output['Actual_SalePrice'] = np.expm1(y_test)\n",
    "cleaned_output['Predicted_SalePrice'] = np.expm1(y_pred_log) # Ensure this uses your final log preds\n",
    "cleaned_output['Error_Amount'] = cleaned_output['Actual_SalePrice'] - cleaned_output['Predicted_SalePrice']\n",
    "\n",
    "# Export to CSV\n",
    "cleaned_output.to_csv(\"Cleaned_Dataset_With_Predictions.csv\", index=False)\n",
    "\n",
    "# 3. Export Feature Importance Rankings\n",
    "best_gbr = models['Safety_Ensemble'].estimators_[1]\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': current_feature_names,\n",
    "    'Importance_Score': best_gbr.feature_importances_\n",
    "}).sort_values(by='Importance_Score', ascending=False)\n",
    "\n",
    "importance_df.to_csv(\"Feature_Importance_Scores.csv\", index=False)\n",
    "\n",
    "print(\"  'Cleaned_Dataset_With_Predictions.csv' and 'Feature_Importance_Scores.csv' are now in folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3816fe1",
   "metadata": {},
   "source": [
    "# Calculate correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9820fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate the correlation matrix\n",
    "corr_matrix = df.select_dtypes(include=['int64', 'float64']).corr()\n",
    "\n",
    "# 2. Get the top 15 features correlated with SalePrice\n",
    "top_corr_features = corr_matrix['SalePrice'].sort_values(ascending=False).head(15).index\n",
    "top_corr_matrix = df[top_corr_features].corr()\n",
    "\n",
    "# 3. Set up the plotting environment\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# 4. Create the heatmap\n",
    "heatmap = sns.heatmap(\n",
    "    top_corr_matrix, \n",
    "    annot=True,          \n",
    "    fmt=\".2f\",           \n",
    "    cmap='coolwarm',     \n",
    "    linewidths=0.5,      \n",
    "    square=True,         \n",
    "    cbar_kws={\"shrink\": .8}\n",
    ")\n",
    "\n",
    "# 5. Add Titles and Labels\n",
    "plt.title('Top 15 Feature Correlations with SalePrice', fontsize=20, pad=20)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "# 6. Save and show\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/correlation_heatmap_full.png', dpi=300) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe38f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare data for the comparison\n",
    "# Load the original raw training data\n",
    "df_raw = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Identify specific high-leverage points (Outliers)\n",
    "# Using the criteria of GrLivArea > 4000 and SalePrice < 300000\n",
    "leverage_points = df_raw[(df_raw['GrLivArea'] > 4000) & (df_raw['SalePrice'] < 300000)]\n",
    "\n",
    "# 2. Configure the visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "# --- Plot A: Initial State (Identification) ---\n",
    "sns.scatterplot(data=df_raw, x='GrLivArea', y='SalePrice', alpha=0.4, ax=ax1, color='#2c3e50', label='Standard Observations')\n",
    "sns.scatterplot(data=leverage_points, x='GrLivArea', y='SalePrice', color='#e74c3c', s=120, marker='D', ax=ax1, label='High-Leverage Observations')\n",
    "ax1.set_title('A: Initial Dataset with Outlier Identification', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Above Ground Living Area (Square Feet)')\n",
    "ax1.set_ylabel('Sale Price (USD)')\n",
    "ax1.legend(frameon=True)\n",
    "\n",
    "# --- Plot B: Post-Filtering State ---\n",
    "# Utilizing 'df' (the processed dataframe) and 'y_final' (the target variable)\n",
    "sns.scatterplot(x=np.expm1(df['GrLivArea']), y=np.expm1(y_final), alpha=0.4, ax=ax2, color='#2980b9')\n",
    "ax2.set_title('B: Dataset Following Outlier Mitigation', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Above Ground Living Area (Square Feet)')\n",
    "ax2.set_ylabel('Sale Price (USD)')\n",
    "\n",
    "# 3. Final Formatting\n",
    "plt.suptitle('Comparison of Dataset Distributions: Pre and Post Observation Filtering', fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save high-resolution version for the report\n",
    "plt.savefig('images/outlier_mitigation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualization successfully saved as 'images/outlier_mitigation_analysis.png'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
